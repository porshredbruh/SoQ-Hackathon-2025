{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных с kaggle. Нужно загрузить kaggle.json с профиля в Kaggle.\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c school-of-quants-hackathon-2025-finals\n",
        "!unzip school-of-quants-hackathon-2025-finals.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwHCC4pUH-x0",
        "outputId": "57f328ad-2883-47ca-b0b5-3761989c2ecd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading school-of-quants-hackathon-2025-finals.zip to /content\n",
            "\r  0% 0.00/43.2M [00:00<?, ?B/s]\n",
            "\r100% 43.2M/43.2M [00:00<00:00, 818MB/s]\n",
            "Archive:  school-of-quants-hackathon-2025-finals.zip\n",
            "  inflating: X_test.csv              \n",
            "  inflating: X_train.csv             \n",
            "  inflating: y_train.csv             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7uMyxYKHqBt",
        "outputId": "62cb44bd-e11d-42aa-f25c-f9b5fdefd319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Shapes: (1827404, 48) (456852, 48) (1827404, 2)\n",
            "Target distribution:\n",
            " flag\n",
            "0    0.967186\n",
            "1    0.032814\n",
            "Name: proportion, dtype: float64\n",
            "Preparing features...\n",
            "Prepared feature shapes: (1827404, 38) (456852, 38)\n",
            "Training with balance_method = undersample\n",
            "Fold 1\n",
            "  Undersampled train: pos=47971, neg=191884 (ratio ~ 4:1)\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\tvalid_0's auc: 0.641402\n",
            "[200]\tvalid_0's auc: 0.643712\n",
            "Early stopping, best iteration is:\n",
            "[212]\tvalid_0's auc: 0.643956\n",
            "Fold ROC AUC: 0.6439562779164065\n",
            "Fold 2\n",
            "  Undersampled train: pos=47971, neg=191884 (ratio ~ 4:1)\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\tvalid_0's auc: 0.634949\n",
            "[200]\tvalid_0's auc: 0.637715\n",
            "[300]\tvalid_0's auc: 0.638687\n",
            "[400]\tvalid_0's auc: 0.63924\n",
            "Early stopping, best iteration is:\n",
            "[428]\tvalid_0's auc: 0.639599\n",
            "Fold ROC AUC: 0.6395986996626063\n",
            "Fold 3\n",
            "  Undersampled train: pos=47971, neg=191884 (ratio ~ 4:1)\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\tvalid_0's auc: 0.640584\n",
            "[200]\tvalid_0's auc: 0.642453\n",
            "[300]\tvalid_0's auc: 0.642524\n",
            "Early stopping, best iteration is:\n",
            "[281]\tvalid_0's auc: 0.642806\n",
            "Fold ROC AUC: 0.6428057553924592\n",
            "Fold 4\n",
            "  Undersampled train: pos=47971, neg=191884 (ratio ~ 4:1)\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\tvalid_0's auc: 0.641684\n",
            "[200]\tvalid_0's auc: 0.643945\n",
            "[300]\tvalid_0's auc: 0.644299\n",
            "Early stopping, best iteration is:\n",
            "[288]\tvalid_0's auc: 0.644712\n",
            "Fold ROC AUC: 0.6447121251871721\n",
            "Fold 5\n",
            "  Undersampled train: pos=47972, neg=191888 (ratio ~ 4:1)\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[100]\tvalid_0's auc: 0.641081\n",
            "[200]\tvalid_0's auc: 0.643501\n",
            "[300]\tvalid_0's auc: 0.644204\n",
            "[400]\tvalid_0's auc: 0.644267\n",
            "Early stopping, best iteration is:\n",
            "[356]\tvalid_0's auc: 0.644364\n",
            "Fold ROC AUC: 0.6443637055808747\n",
            "Best threshold on OOF: 0.30953155182802444 Best OOF F1: 0.11746582688178894\n",
            "Predicting test set...\n",
            "Saved submission.csv, shape: (456852, 2)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Improved training pipeline for default prediction with robust class balancing.\n",
        "\n",
        "Features:\n",
        "- Vectorized feature engineering for enc_paym_* and overdues_ blocks\n",
        "- LightGBM with class imbalance handling: undersample / scale_pos_weight / both\n",
        "- Stratified K-Fold CV + out-of-fold predictions\n",
        "- Threshold tuning to maximize F1\n",
        "- Avoid expensive OneHotEncoder; use LabelEncoder for small categoricals\n",
        "- Memory-friendly transforms where possible\n",
        "\n",
        "Usage:\n",
        "- Place X_train.csv, X_test.csv, y_train.csv in the same folder\n",
        "- Run: python improved_solution_balanced.py\n",
        "Produces: submission.csv with columns ['id', 'flag']\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import f1_score, precision_recall_curve, roc_auc_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "DATA_DIR = \".\"\n",
        "\n",
        "def load_data(data_dir=DATA_DIR):\n",
        "    X_train = pd.read_csv(os.path.join(data_dir, \"X_train.csv\"))\n",
        "    X_test = pd.read_csv(os.path.join(data_dir, \"X_test.csv\"))\n",
        "    y_train = pd.read_csv(os.path.join(data_dir, \"y_train.csv\"))\n",
        "    return X_train, X_test, y_train\n",
        "\n",
        "def basic_checks(X_train, X_test, y_train):\n",
        "    print(\"Shapes:\", X_train.shape, X_test.shape, y_train.shape)\n",
        "    # if y_train contains more than one column, try to find a 'flag' column, else take first\n",
        "    if y_train.shape[1] > 1:\n",
        "        if 'flag' in y_train.columns:\n",
        "            y = y_train['flag'].astype(int)\n",
        "        else:\n",
        "            # fallback: take first column\n",
        "            y = y_train.iloc[:, 0].astype(int)\n",
        "    else:\n",
        "        y = y_train.iloc[:, 0].astype(int)\n",
        "    # print distribution\n",
        "    vc = y.value_counts(normalize=True)\n",
        "    print(\"Target distribution:\\n\", vc.rename(\"proportion\"))\n",
        "    return y\n",
        "\n",
        "# ---------------- Feature engineering ----------------\n",
        "\n",
        "def eng_enc_paym_features(df, enc_cols):\n",
        "    if len(enc_cols) == 0:\n",
        "        return pd.DataFrame(index=df.index)\n",
        "    arr = df[enc_cols].fillna(-99).values  # sentinel for missing\n",
        "    valid_mask = (arr != -99)\n",
        "    # first value (not index)\n",
        "    def first_val(a, vm):\n",
        "        out = np.full(a.shape[0], np.nan, dtype=float)\n",
        "        idx = vm.argmax(axis=1)\n",
        "        any_valid = vm.any(axis=1)\n",
        "        out[any_valid] = a[np.arange(a.shape[0])[any_valid], idx[any_valid]]\n",
        "        return out\n",
        "    first_v = first_val(arr, valid_mask)\n",
        "    # last value\n",
        "    def last_val(a, vm):\n",
        "        out = np.full(a.shape[0], np.nan, dtype=float)\n",
        "        rev_idx = vm[:, ::-1].argmax(axis=1)\n",
        "        any_valid = vm.any(axis=1)\n",
        "        rev_pos = a.shape[1] - 1 - rev_idx\n",
        "        out[any_valid] = a[np.arange(a.shape[0])[any_valid], rev_pos[any_valid]]\n",
        "        return out\n",
        "    last_v = last_val(arr, valid_mask)\n",
        "    mean_v = np.where(valid_mask, arr, np.nan).mean(axis=1)\n",
        "    std_v = np.nanstd(np.where(valid_mask, arr, np.nan), axis=1)\n",
        "    # number of unique statuses (excluding missing)\n",
        "    n_unique = np.apply_along_axis(lambda r: len(np.unique(r[r != -99])), 1, arr)\n",
        "    missing_frac = (arr == -99).mean(axis=1)\n",
        "    diffs = np.abs(np.diff(np.where(arr == -99, np.nan, arr), axis=1))\n",
        "    num_changes = np.nansum((~np.isnan(diffs)) & (diffs > 0), axis=1)\n",
        "    out = pd.DataFrame({\n",
        "        \"enc_first\": first_v,\n",
        "        \"enc_last\": last_v,\n",
        "        \"enc_mean\": mean_v,\n",
        "        \"enc_std\": std_v,\n",
        "        \"enc_n_unique\": n_unique,\n",
        "        \"enc_missing_frac\": missing_frac,\n",
        "        \"enc_num_changes\": num_changes\n",
        "    }, index=df.index)\n",
        "    return out\n",
        "\n",
        "def eng_overdues_features(df, over_cols, no_over_cols):\n",
        "    out = pd.DataFrame(index=df.index)\n",
        "    if len(over_cols) > 0:\n",
        "        arr = df[over_cols].fillna(0).values.astype(float)\n",
        "        out['over_total'] = arr.sum(axis=1)\n",
        "        out['over_num_nonzero'] = (arr > 0).sum(axis=1)\n",
        "        out['over_max_bucket'] = np.argmax(arr, axis=1).astype(int)\n",
        "    else:\n",
        "        out['over_total'] = 0.0\n",
        "        out['over_num_nonzero'] = 0\n",
        "        out['over_max_bucket'] = -1\n",
        "    if len(no_over_cols) > 0:\n",
        "        arr2 = df[no_over_cols].fillna(0).values.astype(float)\n",
        "        out['no_over_total'] = arr2.sum(axis=1)\n",
        "        out['no_over_mean'] = arr2.mean(axis=1)\n",
        "    else:\n",
        "        out['no_over_total'] = 0.0\n",
        "        out['no_over_mean'] = 0.0\n",
        "    return out\n",
        "\n",
        "def num_features_engineer(df):\n",
        "    df = df.copy()\n",
        "    EPS = 1e-9\n",
        "    if 'maturity_plan' in df.columns and 'maturity_fact' in df.columns:\n",
        "        df['maturity_diff'] = df['maturity_fact'] - df['maturity_plan']\n",
        "        df['maturity_ratio'] = df['maturity_fact'] / (df['maturity_plan'] + EPS)\n",
        "        df['closed_early'] = (df['maturity_fact'] < df['maturity_plan']).astype(int)\n",
        "    if 'sum_left_to_pay' in df.columns and 'credit_limit' in df.columns:\n",
        "        df['utilization'] = df['sum_left_to_pay'] / (df['credit_limit'] + EPS)\n",
        "    if 'next_payment_sum' in df.columns and 'credit_limit' in df.columns:\n",
        "        df['next_payment_ratio'] = df['next_payment_sum'] / (df['credit_limit'] + EPS)\n",
        "    if 'current_overdue_debt' in df.columns:\n",
        "        df['has_current_overdue'] = (df['current_overdue_debt'] > 0).astype(int)\n",
        "    money_cols = ['credit_limit','next_payment_sum','sum_left_to_pay','current_overdue_debt','max_overdue_debt','full_credit_cost']\n",
        "    for c in money_cols:\n",
        "        if c in df.columns:\n",
        "            df[c + '_log1p'] = np.log1p(df[c].fillna(0.0).astype(float))\n",
        "    if 'maturity_plan' in df.columns and 'days_since_confirmed' in df.columns:\n",
        "        df['maturity_remaining'] = df['maturity_plan'] - df['days_since_confirmed']\n",
        "    return df\n",
        "\n",
        "def prepare_features(X_train, X_test):\n",
        "    all_cols = X_train.columns.tolist()\n",
        "    enc_paym_cols = [c for c in all_cols if c.startswith('enc_paym_')]\n",
        "    over_cols = [c for c in all_cols if c.startswith('overdues_')]\n",
        "    no_over_cols = [c for c in all_cols if c.startswith('no_overdues_')]\n",
        "    cat_cols = [c for c in ['credit_type','credit_currency'] if c in all_cols]\n",
        "    id_col = 'id' if 'id' in all_cols else None\n",
        "\n",
        "    # numeric engineering\n",
        "    X_train_num = num_features_engineer(X_train)\n",
        "    X_test_num  = num_features_engineer(X_test)\n",
        "\n",
        "    # enc_paym features\n",
        "    enc_tr = eng_enc_paym_features(X_train_num, enc_paym_cols)\n",
        "    enc_te = eng_enc_paym_features(X_test_num, enc_paym_cols)\n",
        "\n",
        "    # overdues features\n",
        "    ov_tr = eng_overdues_features(X_train_num, over_cols, no_over_cols)\n",
        "    ov_te = eng_overdues_features(X_test_num, over_cols, no_over_cols)\n",
        "\n",
        "    # concat and drop original large blocks (to save memory)\n",
        "    drop_cols = enc_paym_cols + over_cols + no_over_cols\n",
        "    X_train_small = pd.concat([X_train_num.drop(columns=[c for c in drop_cols if c in X_train_num.columns]), enc_tr, ov_tr], axis=1)\n",
        "    X_test_small  = pd.concat([X_test_num.drop(columns=[c for c in drop_cols if c in X_test_num.columns]), enc_te, ov_te], axis=1)\n",
        "\n",
        "    # label-encode small categorical columns (inplace)\n",
        "    for c in cat_cols:\n",
        "        le = LabelEncoder()\n",
        "        X_train_small[c] = X_train_small[c].fillna(-999)\n",
        "        X_test_small[c]  = X_test_small[c].fillna(-999)\n",
        "        le.fit(list(X_train_small[c].astype(str).values) + list(X_test_small[c].astype(str).values))\n",
        "        X_train_small[c] = le.transform(X_train_small[c].astype(str))\n",
        "        X_test_small[c]  = le.transform(X_test_small[c].astype(str))\n",
        "\n",
        "    # impute remaining nan with median and cast to float32 for memory\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    X_train_small[:] = imputer.fit_transform(X_train_small)\n",
        "    X_test_small[:]  = imputer.transform(X_test_small)\n",
        "\n",
        "    # convert numeric columns to float32 to save memory\n",
        "    for c in X_train_small.select_dtypes(include=[np.number]).columns:\n",
        "        # categorical columns likely ints; keep them as ints where appropriate\n",
        "        if c in cat_cols:\n",
        "            X_train_small[c] = X_train_small[c].astype('int32')\n",
        "            X_test_small[c] = X_test_small[c].astype('int32')\n",
        "        else:\n",
        "            X_train_small[c] = X_train_small[c].astype('float32')\n",
        "            X_test_small[c] = X_test_small[c].astype('float32')\n",
        "\n",
        "    return X_train_small, X_test_small, id_col, cat_cols\n",
        "\n",
        "# ---------------- Balancing helpers and training ----------------\n",
        "\n",
        "def undersample_negatives_idx(y, desired_neg_ratio=4, random_state=None):\n",
        "    \"\"\"\n",
        "    Return indices to keep: all positives + sample of negatives such that neg:pos ~ desired_neg_ratio.\n",
        "    y is a pd.Series indexed by original indices.\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    idx_all = y.index.values\n",
        "    pos_mask = (y.values == 1)\n",
        "    neg_mask = ~pos_mask\n",
        "    pos_idx = idx_all[pos_mask]\n",
        "    neg_idx = idx_all[neg_mask]\n",
        "\n",
        "    n_pos = pos_idx.shape[0]\n",
        "    n_neg_req = int(n_pos * desired_neg_ratio)\n",
        "    n_neg_req = min(n_neg_req, neg_idx.shape[0])\n",
        "\n",
        "    if n_neg_req <= 0:\n",
        "        return idx_all\n",
        "\n",
        "    sampled_neg = rng.choice(neg_idx, size=n_neg_req, replace=False)\n",
        "    selected = np.concatenate([pos_idx, sampled_neg])\n",
        "    rng.shuffle(selected)\n",
        "    return selected\n",
        "\n",
        "def train_lgb_oof(X, y, cat_cols, n_splits=5, seed=42,\n",
        "                  balance_method='undersample', desired_neg_ratio=4):\n",
        "    \"\"\"\n",
        "    balance_method: 'scale' | 'undersample' | 'both' | 'none'\n",
        "    desired_neg_ratio: negatives per positive when undersampling (e.g. 4)\n",
        "    Returns: (models list, oof_preds)\n",
        "    \"\"\"\n",
        "    base_params = {\n",
        "        'objective': 'binary',\n",
        "        'boosting_type': 'gbdt',\n",
        "        'metric': 'auc',\n",
        "        'learning_rate': 0.05,\n",
        "        'num_leaves': 127,\n",
        "        'n_estimators': 2000,\n",
        "        'random_state': seed,\n",
        "        'n_jobs': -1,\n",
        "        'verbosity': -1\n",
        "    }\n",
        "\n",
        "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    oof_preds = np.zeros(X.shape[0], dtype=float)\n",
        "    models = []\n",
        "\n",
        "    from lightgbm import early_stopping, log_evaluation\n",
        "\n",
        "    for fold, (tr_idx, val_idx) in enumerate(folds.split(X, y)):\n",
        "        print(\"Fold\", fold + 1)\n",
        "        X_tr_full, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
        "        y_tr_full, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
        "\n",
        "        # undersample only training part if requested\n",
        "        if balance_method in ('undersample', 'both'):\n",
        "            sel_idx = undersample_negatives_idx(y_tr_full, desired_neg_ratio=desired_neg_ratio, random_state=seed + fold)\n",
        "            # sel_idx are original indices from y_tr_full.index\n",
        "            X_tr = X_tr_full.loc[sel_idx]\n",
        "            y_tr = y_tr_full.loc[sel_idx]\n",
        "            print(f\"  Undersampled train: pos={int((y_tr==1).sum())}, neg={int((y_tr==0).sum())} (ratio ~ {desired_neg_ratio}:1)\")\n",
        "        else:\n",
        "            X_tr, y_tr = X_tr_full, y_tr_full\n",
        "            print(f\"  Full train used: pos={int((y_tr==1).sum())}, neg={int((y_tr==0).sum())}\")\n",
        "\n",
        "        params = base_params.copy()\n",
        "\n",
        "        # scale_pos_weight if requested\n",
        "        if balance_method in ('scale', 'both'):\n",
        "            pos = int(y_tr.sum())\n",
        "            neg = int(y_tr.shape[0] - pos)\n",
        "            spw = max(1.0, neg / (pos + 1e-9))\n",
        "            params['scale_pos_weight'] = spw\n",
        "            print(f\"  Using scale_pos_weight = {spw:.3f}\")\n",
        "\n",
        "        lgb_train = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_cols, free_raw_data=False)\n",
        "        lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train, categorical_feature=cat_cols, free_raw_data=False)\n",
        "\n",
        "        model = lgb.train(\n",
        "            params,\n",
        "            lgb_train,\n",
        "            valid_sets=[lgb_val],\n",
        "            num_boost_round=params.get('n_estimators', 2000),\n",
        "            callbacks=[early_stopping(stopping_rounds=50), log_evaluation(period=100)]\n",
        "        )\n",
        "\n",
        "        val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "        oof_preds[val_idx] = val_pred\n",
        "        models.append(model)\n",
        "        print(\"Fold ROC AUC:\", roc_auc_score(y_val, val_pred))\n",
        "        gc.collect()\n",
        "\n",
        "    return models, oof_preds\n",
        "\n",
        "def find_best_threshold(y_true, probs):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, probs)\n",
        "    f1_scores = 2 * precision * recall / (precision + recall + 1e-12)\n",
        "    best_idx = np.nanargmax(f1_scores)\n",
        "    best_thr = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
        "    return best_thr, f1_scores[best_idx]\n",
        "\n",
        "def predict_ensemble(models, X, average=True):\n",
        "    preds = np.column_stack([m.predict(X, num_iteration=m.best_iteration) for m in models])\n",
        "    return np.mean(preds, axis=1) if average else preds.mean(axis=1)\n",
        "\n",
        "# ---------------- Main ----------------\n",
        "\n",
        "def main():\n",
        "    required = [\"X_train.csv\", \"X_test.csv\", \"y_train.csv\"]\n",
        "    ok = all(os.path.exists(f) for f in required)\n",
        "    if not ok:\n",
        "        print(\"Не найден один из файлов:\", required)\n",
        "        print(\"Поместите CSV-файлы в текущую папку и запустите снова.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"Loading data...\")\n",
        "    X_train, X_test, y_df = load_data(DATA_DIR)\n",
        "    y = basic_checks(X_train, X_test, y_df)\n",
        "\n",
        "    print(\"Preparing features...\")\n",
        "    X_tr, X_te, id_col, cat_cols = prepare_features(X_train, X_test)\n",
        "    print(\"Prepared feature shapes:\", X_tr.shape, X_te.shape)\n",
        "    # Ensure indexing aligns with y\n",
        "    X_tr = X_tr.loc[y.index]\n",
        "\n",
        "    # TRAIN: try undersample by default; change `balance_method` if you prefer only scale_pos_weight\n",
        "    balance_method = 'undersample'  # options: 'undersample', 'scale', 'both', 'none'\n",
        "    desired_neg_ratio = 4  # negatives per positive in undersampled train set\n",
        "\n",
        "    print(\"Training with balance_method =\", balance_method)\n",
        "    models, oof = train_lgb_oof(X_tr, y, cat_cols, n_splits=5, seed=42,\n",
        "                                balance_method=balance_method, desired_neg_ratio=desired_neg_ratio)\n",
        "\n",
        "    best_thr, best_f1 = find_best_threshold(y, oof)\n",
        "    print(\"Best threshold on OOF:\", best_thr, \"Best OOF F1:\", best_f1)\n",
        "\n",
        "    # Final predictions for test\n",
        "    print(\"Predicting test set...\")\n",
        "    test_probs = predict_ensemble(models, X_te)\n",
        "    test_pred = (test_probs >= best_thr).astype(int)\n",
        "\n",
        "    if id_col is None or id_col not in X_test.columns:\n",
        "        sub_ids = np.arange(len(test_pred))\n",
        "    else:\n",
        "        sub_ids = X_test[id_col].astype(int).values\n",
        "\n",
        "    submission = pd.DataFrame({\"id\": sub_ids, \"flag\": test_pred})\n",
        "    submission.to_csv(\"submission.csv\", index=False)\n",
        "    print(\"Saved submission.csv, shape:\", submission.shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWtnC9DlIPLU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}