{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYhlkeZtuaWC"
      },
      "outputs": [],
      "source": [
        "# transformer_forecast.py\n",
        "# Transformer Encoder for 13-step ahead forecasting on OHLC data.\n",
        "# Usage:\n",
        "#   python transformer_forecast.py --train train.csv --test test.csv --output submission.csv\n",
        "# Optional:\n",
        "#   --seq_len 90 --d_model 256 --nhead 8 --num_layers 6 --epochs 120\n",
        "#   --cpu  (force CPU even if CUDA is available)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "HORIZON = 13"
      ],
      "metadata": {
        "id": "dgiKGVrFEDIv"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Датасет ---\n",
        "class WindowDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray, seq_len: int):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(0, self.X.shape[0] - self.seq_len - HORIZON + 1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x_seq = self.X[idx: idx + self.seq_len]\n",
        "        y_vec = self.y[idx + self.seq_len: idx + self.seq_len + HORIZON]\n",
        "        return torch.from_numpy(x_seq).float(), torch.from_numpy(y_vec).float()\n"
      ],
      "metadata": {
        "id": "WW5IhyQ6EFnx"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Фичи ----------\n",
        "def add_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out[\"range_hl\"] = out[\"high\"] - out[\"low\"]\n",
        "    out[\"close_shift1\"] = out[\"close\"].shift(1)\n",
        "    out[\"ret1\"] = np.log(out[\"close\"] / out[\"close_shift1\"]).replace([np.inf, -np.inf], 0.0).fillna(0.0)\n",
        "    out[\"dow\"] = out[\"dt\"].dt.dayofweek\n",
        "    out[\"dow_sin\"] = np.sin(2*np.pi*out[\"dow\"]/7.0)\n",
        "    out[\"dow_cos\"] = np.cos(2*np.pi*out[\"dow\"]/7.0)\n",
        "    for w in [3, 5, 7, 13]:\n",
        "        out[f\"sma_{w}\"] = out[\"close\"].rolling(w, min_periods=1).mean()\n",
        "    out = out.drop(columns=[\"close_shift1\"])\n",
        "    return out"
      ],
      "metadata": {
        "id": "QdEUJ_OeEItW"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Модель ---\n",
        "class LSTMForecast(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=16, num_layers=1, horizon=13):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
        "                            batch_first=True, dropout=0.0)\n",
        "        self.fc = nn.Linear(hidden_dim, horizon)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (h, _) = self.lstm(x)\n",
        "        h = h[-1]\n",
        "        return self.fc(h)"
      ],
      "metadata": {
        "id": "njwUrQfDELLO"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Функции подготовки ---\n",
        "def make_arrays(df, feature_cols, target_col,\n",
        "                feat_scaler=None, target_scaler=None):\n",
        "    X = df[feature_cols].values.astype(np.float32)\n",
        "    y = df[target_col].values.astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "    if feat_scaler is None:\n",
        "        feat_scaler = StandardScaler()\n",
        "        X = feat_scaler.fit_transform(X)\n",
        "    else:\n",
        "        X = feat_scaler.transform(X)\n",
        "\n",
        "    if target_scaler is None:\n",
        "        target_scaler = StandardScaler()\n",
        "        y = target_scaler.fit_transform(y)\n",
        "    else:\n",
        "        y = target_scaler.transform(y)\n",
        "\n",
        "    return X, y.squeeze(-1), feat_scaler, target_scaler\n",
        "\n",
        "\n",
        "def predict_last_window(model, X, seq_len, device):\n",
        "    x_seq = torch.from_numpy(X[-seq_len:]).unsqueeze(0).float().to(device)\n",
        "    with torch.no_grad():\n",
        "        pred = model(x_seq).cpu().numpy().ravel()\n",
        "    return pred"
      ],
      "metadata": {
        "id": "_kEgeUS_ENUB"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.cpu else \"cpu\")\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # данные\n",
        "    train_df = pd.read_csv(\"train.csv\")\n",
        "    test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "    # --- преобразуем дату ---\n",
        "    date_col = \"dt\"\n",
        "\n",
        "    for df in [train_df, test_df]:\n",
        "        # Преобразуем колонку dt в datetime\n",
        "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')  # некорректные даты станут NaT\n",
        "\n",
        "        # Проверим, что преобразование прошло успешно\n",
        "        if df[date_col].isna().any():\n",
        "            print(\"В колонке dt есть некорректные даты!\")\n",
        "\n",
        "        # Создаём признаки из даты\n",
        "        df[\"year\"] = df[date_col].dt.year\n",
        "        df[\"month\"] = df[date_col].dt.month\n",
        "        df[\"day\"] = df[date_col].dt.day\n",
        "        df[\"weekday\"] = df[date_col].dt.weekday\n",
        "\n",
        "    # Исключаем колонку даты из признаков\n",
        "    feature_cols = [c for c in train_df.columns if c not in [\"close\", date_col]]\n",
        "    target_col = \"close\"\n",
        "\n",
        "    X_all, y_all, feat_scaler, target_scaler = make_arrays(train_df, feature_cols, \"close\")\n",
        "\n",
        "    # проверяем достаточно ли данных\n",
        "    min_required = args.seq_len + HORIZON\n",
        "    if len(train_df) < min_required:\n",
        "        new_seq_len = max(5, len(train_df) - HORIZON - 1)\n",
        "        print(f\"⚠️ Данных мало ({len(train_df)}). seq_len уменьшаем до {new_seq_len}\")\n",
        "        args.seq_len = new_seq_len\n",
        "\n",
        "    # делим train/val\n",
        "    dataset = WindowDataset(X_all, y_all, args.seq_len)\n",
        "    n_total = len(dataset)\n",
        "    n_val = max(1, int(n_total * args.val_ratio))\n",
        "    n_train = max(1, n_total - n_val)\n",
        "    train_ds, val_ds = random_split(dataset, [n_train, n_val])\n",
        "\n",
        "    print(f\"Train size: {len(train_ds)}, Test size: {len(val_ds)}\")\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=args.batch_size)\n",
        "\n",
        "    # модель\n",
        "    model = LSTMForecast(len(feature_cols), hidden_dim=args.hidden_dim,\n",
        "                         num_layers=args.num_layers, horizon=HORIZON).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "    crit = nn.MSELoss()\n",
        "\n",
        "    # обучение\n",
        "    best_val = float(\"inf\")\n",
        "    patience = args.patience\n",
        "\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            pred = model(xb)\n",
        "            loss = crit(pred, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        val_losses = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                pred = model(xb)\n",
        "                loss = crit(pred, yb)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.mean(train_losses) if train_losses else 0.0\n",
        "        val_loss = np.mean(val_losses) if val_losses else float(\"inf\")\n",
        "\n",
        "        if val_loss < best_val:\n",
        "            best_val = val_loss\n",
        "            patience = args.patience\n",
        "            torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        else:\n",
        "            patience -= 1\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.6f} | \"\n",
        "              f\"val_loss={val_loss:.6f} | best_val={best_val:.6f} | patience={patience}\")\n",
        "\n",
        "        if patience == 0:\n",
        "            print(\"Ранняя остановка.\")\n",
        "            break\n",
        "\n",
        "    # загрузка лучшей модели\n",
        "    model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "\n",
        "    # предсказания для последнего окна\n",
        "    preds_scaled = predict_last_window(model, X_all, args.seq_len, device)\n",
        "    preds = target_scaler.inverse_transform(preds_scaled.reshape(-1, 1)).ravel()\n",
        "\n",
        "    # сабмит\n",
        "    submission = pd.DataFrame({\"id\": np.arange(1, len(preds) + 1),\n",
        "                               \"close\": preds})\n",
        "    submission.to_csv(\"submission.csv\", index=False)\n",
        "    print(\"Сабмит сохранён в submission.csv\")"
      ],
      "metadata": {
        "id": "OMZmBU2tEPUk"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--seq_len\", type=int, default=20)\n",
        "    parser.add_argument(\"--hidden_dim\", type=int, default=16)\n",
        "    parser.add_argument(\"--num_layers\", type=int, default=1)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=200)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--val_ratio\", type=float, default=0.2)\n",
        "    parser.add_argument(\"--patience\", type=int, default=20)\n",
        "    parser.add_argument(\"--cpu\", action=\"store_true\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pz5hQxVQERdo",
        "outputId": "864aebd9-bd7e-415a-ff87-5c40e3c0be6f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Train size: 25, Test size: 6\n",
            "Epoch 001 | train_loss=1.079144 | val_loss=0.801465 | best_val=0.801465 | patience=20\n",
            "Epoch 002 | train_loss=0.894695 | val_loss=0.790285 | best_val=0.790285 | patience=20\n",
            "Epoch 003 | train_loss=0.825249 | val_loss=0.778576 | best_val=0.778576 | patience=20\n",
            "Epoch 004 | train_loss=1.223972 | val_loss=0.767093 | best_val=0.767093 | patience=20\n",
            "Epoch 005 | train_loss=0.826729 | val_loss=0.754578 | best_val=0.754578 | patience=20\n",
            "Epoch 006 | train_loss=1.005680 | val_loss=0.740771 | best_val=0.740771 | patience=20\n",
            "Epoch 007 | train_loss=0.967917 | val_loss=0.725271 | best_val=0.725271 | patience=20\n",
            "Epoch 008 | train_loss=1.071183 | val_loss=0.708217 | best_val=0.708217 | patience=20\n",
            "Epoch 009 | train_loss=0.743105 | val_loss=0.689545 | best_val=0.689545 | patience=20\n",
            "Epoch 010 | train_loss=0.905349 | val_loss=0.670112 | best_val=0.670112 | patience=20\n",
            "Epoch 011 | train_loss=0.981699 | val_loss=0.647173 | best_val=0.647173 | patience=20\n",
            "Epoch 012 | train_loss=0.790101 | val_loss=0.621691 | best_val=0.621691 | patience=20\n",
            "Epoch 013 | train_loss=0.729376 | val_loss=0.594962 | best_val=0.594962 | patience=20\n",
            "Epoch 014 | train_loss=0.697042 | val_loss=0.566700 | best_val=0.566700 | patience=20\n",
            "Epoch 015 | train_loss=0.594859 | val_loss=0.538613 | best_val=0.538613 | patience=20\n",
            "Epoch 016 | train_loss=0.643255 | val_loss=0.511682 | best_val=0.511682 | patience=20\n",
            "Epoch 017 | train_loss=0.490732 | val_loss=0.483072 | best_val=0.483072 | patience=20\n",
            "Epoch 018 | train_loss=0.494983 | val_loss=0.455607 | best_val=0.455607 | patience=20\n",
            "Epoch 019 | train_loss=0.446151 | val_loss=0.429920 | best_val=0.429920 | patience=20\n",
            "Epoch 020 | train_loss=0.445942 | val_loss=0.404154 | best_val=0.404154 | patience=20\n",
            "Epoch 021 | train_loss=0.341283 | val_loss=0.373508 | best_val=0.373508 | patience=20\n",
            "Epoch 022 | train_loss=0.303773 | val_loss=0.342500 | best_val=0.342500 | patience=20\n",
            "Epoch 023 | train_loss=0.310593 | val_loss=0.311989 | best_val=0.311989 | patience=20\n",
            "Epoch 024 | train_loss=0.260583 | val_loss=0.286449 | best_val=0.286449 | patience=20\n",
            "Epoch 025 | train_loss=0.252495 | val_loss=0.265425 | best_val=0.265425 | patience=20\n",
            "Epoch 026 | train_loss=0.243561 | val_loss=0.248256 | best_val=0.248256 | patience=20\n",
            "Epoch 027 | train_loss=0.188580 | val_loss=0.231977 | best_val=0.231977 | patience=20\n",
            "Epoch 028 | train_loss=0.176053 | val_loss=0.219613 | best_val=0.219613 | patience=20\n",
            "Epoch 029 | train_loss=0.153417 | val_loss=0.209202 | best_val=0.209202 | patience=20\n",
            "Epoch 030 | train_loss=0.190141 | val_loss=0.199819 | best_val=0.199819 | patience=20\n",
            "Epoch 031 | train_loss=0.138575 | val_loss=0.190744 | best_val=0.190744 | patience=20\n",
            "Epoch 032 | train_loss=0.193116 | val_loss=0.183336 | best_val=0.183336 | patience=20\n",
            "Epoch 033 | train_loss=0.146919 | val_loss=0.177593 | best_val=0.177593 | patience=20\n",
            "Epoch 034 | train_loss=0.157676 | val_loss=0.171977 | best_val=0.171977 | patience=20\n",
            "Epoch 035 | train_loss=0.128352 | val_loss=0.165526 | best_val=0.165526 | patience=20\n",
            "Epoch 036 | train_loss=0.134747 | val_loss=0.160932 | best_val=0.160932 | patience=20\n",
            "Epoch 037 | train_loss=0.173786 | val_loss=0.156443 | best_val=0.156443 | patience=20\n",
            "Epoch 038 | train_loss=0.102294 | val_loss=0.160025 | best_val=0.156443 | patience=19\n",
            "Epoch 039 | train_loss=0.128734 | val_loss=0.160440 | best_val=0.156443 | patience=18\n",
            "Epoch 040 | train_loss=0.100441 | val_loss=0.150041 | best_val=0.150041 | patience=20\n",
            "Epoch 041 | train_loss=0.108557 | val_loss=0.142042 | best_val=0.142042 | patience=20\n",
            "Epoch 042 | train_loss=0.104621 | val_loss=0.135050 | best_val=0.135050 | patience=20\n",
            "Epoch 043 | train_loss=0.105452 | val_loss=0.129769 | best_val=0.129769 | patience=20\n",
            "Epoch 044 | train_loss=0.094088 | val_loss=0.128221 | best_val=0.128221 | patience=20\n",
            "Epoch 045 | train_loss=0.135782 | val_loss=0.132082 | best_val=0.128221 | patience=19\n",
            "Epoch 046 | train_loss=0.101625 | val_loss=0.144451 | best_val=0.128221 | patience=18\n",
            "Epoch 047 | train_loss=0.082517 | val_loss=0.141504 | best_val=0.128221 | patience=17\n",
            "Epoch 048 | train_loss=0.095711 | val_loss=0.130627 | best_val=0.128221 | patience=16\n",
            "Epoch 049 | train_loss=0.075734 | val_loss=0.117387 | best_val=0.117387 | patience=20\n",
            "Epoch 050 | train_loss=0.071902 | val_loss=0.109102 | best_val=0.109102 | patience=20\n",
            "Epoch 051 | train_loss=0.085882 | val_loss=0.105371 | best_val=0.105371 | patience=20\n",
            "Epoch 052 | train_loss=0.118143 | val_loss=0.106961 | best_val=0.105371 | patience=19\n",
            "Epoch 053 | train_loss=0.073131 | val_loss=0.115440 | best_val=0.105371 | patience=18\n",
            "Epoch 054 | train_loss=0.079826 | val_loss=0.114178 | best_val=0.105371 | patience=17\n",
            "Epoch 055 | train_loss=0.099146 | val_loss=0.105361 | best_val=0.105361 | patience=20\n",
            "Epoch 056 | train_loss=0.096395 | val_loss=0.096255 | best_val=0.096255 | patience=20\n",
            "Epoch 057 | train_loss=0.074878 | val_loss=0.086750 | best_val=0.086750 | patience=20\n",
            "Epoch 058 | train_loss=0.074583 | val_loss=0.084866 | best_val=0.084866 | patience=20\n",
            "Epoch 059 | train_loss=0.062087 | val_loss=0.085263 | best_val=0.084866 | patience=19\n",
            "Epoch 060 | train_loss=0.058813 | val_loss=0.087295 | best_val=0.084866 | patience=18\n",
            "Epoch 061 | train_loss=0.075606 | val_loss=0.087559 | best_val=0.084866 | patience=17\n",
            "Epoch 062 | train_loss=0.072428 | val_loss=0.085263 | best_val=0.084866 | patience=16\n",
            "Epoch 063 | train_loss=0.054941 | val_loss=0.081032 | best_val=0.081032 | patience=20\n",
            "Epoch 064 | train_loss=0.053329 | val_loss=0.077975 | best_val=0.077975 | patience=20\n",
            "Epoch 065 | train_loss=0.069817 | val_loss=0.076174 | best_val=0.076174 | patience=20\n",
            "Epoch 066 | train_loss=0.054408 | val_loss=0.075090 | best_val=0.075090 | patience=20\n",
            "Epoch 067 | train_loss=0.050088 | val_loss=0.074556 | best_val=0.074556 | patience=20\n",
            "Epoch 068 | train_loss=0.064525 | val_loss=0.073772 | best_val=0.073772 | patience=20\n",
            "Epoch 069 | train_loss=0.050102 | val_loss=0.073337 | best_val=0.073337 | patience=20\n",
            "Epoch 070 | train_loss=0.065319 | val_loss=0.071985 | best_val=0.071985 | patience=20\n",
            "Epoch 071 | train_loss=0.058916 | val_loss=0.070589 | best_val=0.070589 | patience=20\n",
            "Epoch 072 | train_loss=0.061394 | val_loss=0.069735 | best_val=0.069735 | patience=20\n",
            "Epoch 073 | train_loss=0.047748 | val_loss=0.068861 | best_val=0.068861 | patience=20\n",
            "Epoch 074 | train_loss=0.045986 | val_loss=0.067127 | best_val=0.067127 | patience=20\n",
            "Epoch 075 | train_loss=0.066633 | val_loss=0.065435 | best_val=0.065435 | patience=20\n",
            "Epoch 076 | train_loss=0.060225 | val_loss=0.063339 | best_val=0.063339 | patience=20\n",
            "Epoch 077 | train_loss=0.046763 | val_loss=0.062365 | best_val=0.062365 | patience=20\n",
            "Epoch 078 | train_loss=0.059211 | val_loss=0.061810 | best_val=0.061810 | patience=20\n",
            "Epoch 079 | train_loss=0.043831 | val_loss=0.061292 | best_val=0.061292 | patience=20\n",
            "Epoch 080 | train_loss=0.043712 | val_loss=0.060567 | best_val=0.060567 | patience=20\n",
            "Epoch 081 | train_loss=0.041650 | val_loss=0.059960 | best_val=0.059960 | patience=20\n",
            "Epoch 082 | train_loss=0.054164 | val_loss=0.059398 | best_val=0.059398 | patience=20\n",
            "Epoch 083 | train_loss=0.040737 | val_loss=0.058661 | best_val=0.058661 | patience=20\n",
            "Epoch 084 | train_loss=0.056892 | val_loss=0.058025 | best_val=0.058025 | patience=20\n",
            "Epoch 085 | train_loss=0.040006 | val_loss=0.058113 | best_val=0.058025 | patience=19\n",
            "Epoch 086 | train_loss=0.040114 | val_loss=0.057084 | best_val=0.057084 | patience=20\n",
            "Epoch 087 | train_loss=0.056535 | val_loss=0.056080 | best_val=0.056080 | patience=20\n",
            "Epoch 088 | train_loss=0.059404 | val_loss=0.056200 | best_val=0.056080 | patience=19\n",
            "Epoch 089 | train_loss=0.038337 | val_loss=0.056367 | best_val=0.056080 | patience=18\n",
            "Epoch 090 | train_loss=0.056505 | val_loss=0.056267 | best_val=0.056080 | patience=17\n",
            "Epoch 091 | train_loss=0.035907 | val_loss=0.055977 | best_val=0.055977 | patience=20\n",
            "Epoch 092 | train_loss=0.037581 | val_loss=0.055370 | best_val=0.055370 | patience=20\n",
            "Epoch 093 | train_loss=0.049498 | val_loss=0.054931 | best_val=0.054931 | patience=20\n",
            "Epoch 094 | train_loss=0.052465 | val_loss=0.054986 | best_val=0.054931 | patience=19\n",
            "Epoch 095 | train_loss=0.054121 | val_loss=0.054634 | best_val=0.054634 | patience=20\n",
            "Epoch 096 | train_loss=0.050235 | val_loss=0.054071 | best_val=0.054071 | patience=20\n",
            "Epoch 097 | train_loss=0.036536 | val_loss=0.053545 | best_val=0.053545 | patience=20\n",
            "Epoch 098 | train_loss=0.035934 | val_loss=0.053395 | best_val=0.053395 | patience=20\n",
            "Epoch 099 | train_loss=0.047725 | val_loss=0.053297 | best_val=0.053297 | patience=20\n",
            "Epoch 100 | train_loss=0.053778 | val_loss=0.053288 | best_val=0.053288 | patience=20\n",
            "Epoch 101 | train_loss=0.034236 | val_loss=0.053355 | best_val=0.053288 | patience=19\n",
            "Epoch 102 | train_loss=0.042444 | val_loss=0.053222 | best_val=0.053222 | patience=20\n",
            "Epoch 103 | train_loss=0.046344 | val_loss=0.053555 | best_val=0.053222 | patience=19\n",
            "Epoch 104 | train_loss=0.045068 | val_loss=0.053997 | best_val=0.053222 | patience=18\n",
            "Epoch 105 | train_loss=0.046172 | val_loss=0.053650 | best_val=0.053222 | patience=17\n",
            "Epoch 106 | train_loss=0.034028 | val_loss=0.053038 | best_val=0.053038 | patience=20\n",
            "Epoch 107 | train_loss=0.034140 | val_loss=0.053021 | best_val=0.053021 | patience=20\n",
            "Epoch 108 | train_loss=0.042465 | val_loss=0.052666 | best_val=0.052666 | patience=20\n",
            "Epoch 109 | train_loss=0.038019 | val_loss=0.052427 | best_val=0.052427 | patience=20\n",
            "Epoch 110 | train_loss=0.034908 | val_loss=0.052568 | best_val=0.052427 | patience=19\n",
            "Epoch 111 | train_loss=0.041152 | val_loss=0.052860 | best_val=0.052427 | patience=18\n",
            "Epoch 112 | train_loss=0.033706 | val_loss=0.053163 | best_val=0.052427 | patience=17\n",
            "Epoch 113 | train_loss=0.051681 | val_loss=0.053292 | best_val=0.052427 | patience=16\n",
            "Epoch 114 | train_loss=0.033107 | val_loss=0.053679 | best_val=0.052427 | patience=15\n",
            "Epoch 115 | train_loss=0.032966 | val_loss=0.053387 | best_val=0.052427 | patience=14\n",
            "Epoch 116 | train_loss=0.033114 | val_loss=0.053021 | best_val=0.052427 | patience=13\n",
            "Epoch 117 | train_loss=0.048816 | val_loss=0.053149 | best_val=0.052427 | patience=12\n",
            "Epoch 118 | train_loss=0.039408 | val_loss=0.053666 | best_val=0.052427 | patience=11\n",
            "Epoch 119 | train_loss=0.047369 | val_loss=0.052869 | best_val=0.052427 | patience=10\n",
            "Epoch 120 | train_loss=0.030153 | val_loss=0.051556 | best_val=0.051556 | patience=20\n",
            "Epoch 121 | train_loss=0.038593 | val_loss=0.051908 | best_val=0.051556 | patience=19\n",
            "Epoch 122 | train_loss=0.030852 | val_loss=0.050900 | best_val=0.050900 | patience=20\n",
            "Epoch 123 | train_loss=0.029358 | val_loss=0.050582 | best_val=0.050582 | patience=20\n",
            "Epoch 124 | train_loss=0.040599 | val_loss=0.050386 | best_val=0.050386 | patience=20\n",
            "Epoch 125 | train_loss=0.028359 | val_loss=0.050756 | best_val=0.050386 | patience=19\n",
            "Epoch 126 | train_loss=0.045168 | val_loss=0.050743 | best_val=0.050386 | patience=18\n",
            "Epoch 127 | train_loss=0.045654 | val_loss=0.050983 | best_val=0.050386 | patience=17\n",
            "Epoch 128 | train_loss=0.036608 | val_loss=0.051136 | best_val=0.050386 | patience=16\n",
            "Epoch 129 | train_loss=0.049134 | val_loss=0.051167 | best_val=0.050386 | patience=15\n",
            "Epoch 130 | train_loss=0.029848 | val_loss=0.050763 | best_val=0.050386 | patience=14\n",
            "Epoch 131 | train_loss=0.035483 | val_loss=0.050447 | best_val=0.050386 | patience=13\n",
            "Epoch 132 | train_loss=0.029127 | val_loss=0.050239 | best_val=0.050239 | patience=20\n",
            "Epoch 133 | train_loss=0.027166 | val_loss=0.050028 | best_val=0.050028 | patience=20\n",
            "Epoch 134 | train_loss=0.033560 | val_loss=0.049913 | best_val=0.049913 | patience=20\n",
            "Epoch 135 | train_loss=0.027568 | val_loss=0.049951 | best_val=0.049913 | patience=19\n",
            "Epoch 136 | train_loss=0.028226 | val_loss=0.049976 | best_val=0.049913 | patience=18\n",
            "Epoch 137 | train_loss=0.042550 | val_loss=0.050001 | best_val=0.049913 | patience=17\n",
            "Epoch 138 | train_loss=0.039845 | val_loss=0.050329 | best_val=0.049913 | patience=16\n",
            "Epoch 139 | train_loss=0.027584 | val_loss=0.049880 | best_val=0.049880 | patience=20\n",
            "Epoch 140 | train_loss=0.027322 | val_loss=0.049390 | best_val=0.049390 | patience=20\n",
            "Epoch 141 | train_loss=0.035001 | val_loss=0.049075 | best_val=0.049075 | patience=20\n",
            "Epoch 142 | train_loss=0.035125 | val_loss=0.049206 | best_val=0.049075 | patience=19\n",
            "Epoch 143 | train_loss=0.026876 | val_loss=0.049167 | best_val=0.049075 | patience=18\n",
            "Epoch 144 | train_loss=0.031742 | val_loss=0.049282 | best_val=0.049075 | patience=17\n",
            "Epoch 145 | train_loss=0.042270 | val_loss=0.049693 | best_val=0.049075 | patience=16\n",
            "Epoch 146 | train_loss=0.025429 | val_loss=0.049569 | best_val=0.049075 | patience=15\n",
            "Epoch 147 | train_loss=0.026674 | val_loss=0.049148 | best_val=0.049075 | patience=14\n",
            "Epoch 148 | train_loss=0.026231 | val_loss=0.048828 | best_val=0.048828 | patience=20\n",
            "Epoch 149 | train_loss=0.041011 | val_loss=0.048539 | best_val=0.048539 | patience=20\n",
            "Epoch 150 | train_loss=0.039641 | val_loss=0.047960 | best_val=0.047960 | patience=20\n",
            "Epoch 151 | train_loss=0.033430 | val_loss=0.047820 | best_val=0.047820 | patience=20\n",
            "Epoch 152 | train_loss=0.025483 | val_loss=0.047430 | best_val=0.047430 | patience=20\n",
            "Epoch 153 | train_loss=0.025478 | val_loss=0.047312 | best_val=0.047312 | patience=20\n",
            "Epoch 154 | train_loss=0.036395 | val_loss=0.047498 | best_val=0.047312 | patience=19\n",
            "Epoch 155 | train_loss=0.025048 | val_loss=0.048070 | best_val=0.047312 | patience=18\n",
            "Epoch 156 | train_loss=0.029620 | val_loss=0.048192 | best_val=0.047312 | patience=17\n",
            "Epoch 157 | train_loss=0.024760 | val_loss=0.048109 | best_val=0.047312 | patience=16\n",
            "Epoch 158 | train_loss=0.035946 | val_loss=0.047966 | best_val=0.047312 | patience=15\n",
            "Epoch 159 | train_loss=0.024447 | val_loss=0.047624 | best_val=0.047312 | patience=14\n",
            "Epoch 160 | train_loss=0.037956 | val_loss=0.047399 | best_val=0.047312 | patience=13\n",
            "Epoch 161 | train_loss=0.040829 | val_loss=0.046932 | best_val=0.046932 | patience=20\n",
            "Epoch 162 | train_loss=0.024023 | val_loss=0.046140 | best_val=0.046140 | patience=20\n",
            "Epoch 163 | train_loss=0.023642 | val_loss=0.045545 | best_val=0.045545 | patience=20\n",
            "Epoch 164 | train_loss=0.031502 | val_loss=0.045440 | best_val=0.045440 | patience=20\n",
            "Epoch 165 | train_loss=0.028418 | val_loss=0.045618 | best_val=0.045440 | patience=19\n",
            "Epoch 166 | train_loss=0.023697 | val_loss=0.045235 | best_val=0.045235 | patience=20\n",
            "Epoch 167 | train_loss=0.023907 | val_loss=0.045346 | best_val=0.045235 | patience=19\n",
            "Epoch 168 | train_loss=0.027764 | val_loss=0.045725 | best_val=0.045235 | patience=18\n",
            "Epoch 169 | train_loss=0.026310 | val_loss=0.047326 | best_val=0.045235 | patience=17\n",
            "Epoch 170 | train_loss=0.030862 | val_loss=0.047932 | best_val=0.045235 | patience=16\n",
            "Epoch 171 | train_loss=0.022855 | val_loss=0.046980 | best_val=0.045235 | patience=15\n",
            "Epoch 172 | train_loss=0.022867 | val_loss=0.047043 | best_val=0.045235 | patience=14\n",
            "Epoch 173 | train_loss=0.034322 | val_loss=0.047022 | best_val=0.045235 | patience=13\n",
            "Epoch 174 | train_loss=0.032949 | val_loss=0.047142 | best_val=0.045235 | patience=12\n",
            "Epoch 175 | train_loss=0.022494 | val_loss=0.046671 | best_val=0.045235 | patience=11\n",
            "Epoch 176 | train_loss=0.035025 | val_loss=0.045641 | best_val=0.045235 | patience=10\n",
            "Epoch 177 | train_loss=0.023259 | val_loss=0.044881 | best_val=0.044881 | patience=20\n",
            "Epoch 178 | train_loss=0.032108 | val_loss=0.044558 | best_val=0.044558 | patience=20\n",
            "Epoch 179 | train_loss=0.029971 | val_loss=0.044847 | best_val=0.044558 | patience=19\n",
            "Epoch 180 | train_loss=0.022039 | val_loss=0.044930 | best_val=0.044558 | patience=18\n",
            "Epoch 181 | train_loss=0.021973 | val_loss=0.044740 | best_val=0.044558 | patience=17\n",
            "Epoch 182 | train_loss=0.026188 | val_loss=0.044492 | best_val=0.044492 | patience=20\n",
            "Epoch 183 | train_loss=0.025310 | val_loss=0.044433 | best_val=0.044433 | patience=20\n",
            "Epoch 184 | train_loss=0.022379 | val_loss=0.044598 | best_val=0.044433 | patience=19\n",
            "Epoch 185 | train_loss=0.025461 | val_loss=0.044839 | best_val=0.044433 | patience=18\n",
            "Epoch 186 | train_loss=0.021487 | val_loss=0.045434 | best_val=0.044433 | patience=17\n",
            "Epoch 187 | train_loss=0.022190 | val_loss=0.045637 | best_val=0.044433 | patience=16\n",
            "Epoch 188 | train_loss=0.022047 | val_loss=0.045129 | best_val=0.044433 | patience=15\n",
            "Epoch 189 | train_loss=0.021776 | val_loss=0.044741 | best_val=0.044433 | patience=14\n",
            "Epoch 190 | train_loss=0.032707 | val_loss=0.044581 | best_val=0.044433 | patience=13\n",
            "Epoch 191 | train_loss=0.024586 | val_loss=0.044436 | best_val=0.044433 | patience=12\n",
            "Epoch 192 | train_loss=0.024497 | val_loss=0.044662 | best_val=0.044433 | patience=11\n",
            "Epoch 193 | train_loss=0.030937 | val_loss=0.046177 | best_val=0.044433 | patience=10\n",
            "Epoch 194 | train_loss=0.021671 | val_loss=0.046286 | best_val=0.044433 | patience=9\n",
            "Epoch 195 | train_loss=0.029433 | val_loss=0.044794 | best_val=0.044433 | patience=8\n",
            "Epoch 196 | train_loss=0.033319 | val_loss=0.044122 | best_val=0.044122 | patience=20\n",
            "Epoch 197 | train_loss=0.031726 | val_loss=0.044291 | best_val=0.044122 | patience=19\n",
            "Epoch 198 | train_loss=0.026640 | val_loss=0.043669 | best_val=0.043669 | patience=20\n",
            "Epoch 199 | train_loss=0.021567 | val_loss=0.042834 | best_val=0.042834 | patience=20\n",
            "Epoch 200 | train_loss=0.032196 | val_loss=0.042570 | best_val=0.042570 | patience=20\n",
            "Сабмит сохранён в submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2kVTJtUnErxC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}