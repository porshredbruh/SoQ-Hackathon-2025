{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Загрузка данных с kaggle. Нужно загрузить kaggle.json с профиля в Kaggle.\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c school-of-quants-hackathon-2025-finals\n",
        "!unzip school-of-quants-hackathon-2025-finals.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwHCC4pUH-x0",
        "outputId": "57f328ad-2883-47ca-b0b5-3761989c2ecd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading school-of-quants-hackathon-2025-finals.zip to /content\n",
            "\r  0% 0.00/43.2M [00:00<?, ?B/s]\n",
            "\r100% 43.2M/43.2M [00:00<00:00, 818MB/s]\n",
            "Archive:  school-of-quants-hackathon-2025-finals.zip\n",
            "  inflating: X_test.csv              \n",
            "  inflating: X_train.csv             \n",
            "  inflating: y_train.csv             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7uMyxYKHqBt",
        "outputId": "880e4876-e9df-4336-f9fe-757ef3be21c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading...\n",
            "Train shape: (1827404, 48) Test shape: (456852, 48) Y shape: (1827404,)\n",
            "Target distribution: {0: 0.9671862379638, 1: 0.032813762036199984}\n",
            "Feature engineering...\n",
            "Mem usage reduced from 456.60 MB to 170.79 MB\n",
            "Mem usage reduced from 114.15 MB to 42.70 MB\n",
            "Final feature shape: (1827404, 41) Categorical features: ['credit_type_le', 'credit_currency_le']\n",
            "Training bagging ensemble...\n",
            "\n",
            "=== BAG 1/5 ===\n",
            "Bag train shape: (299820, 41) Pos: 59964 Neg: 239856\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[676]\tvalid_0's auc: 0.646679\n",
            "Bag 1 Fold 1: val ROC AUC 0.6467\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[1024]\tvalid_0's auc: 0.648741\n",
            "Bag 1 Fold 2: val ROC AUC 0.6487\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[731]\tvalid_0's auc: 0.644478\n",
            "Bag 1 Fold 3: val ROC AUC 0.6445\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[892]\tvalid_0's auc: 0.642895\n",
            "Bag 1 Fold 4: val ROC AUC 0.6429\n",
            "\n",
            "=== BAG 2/5 ===\n",
            "Bag train shape: (299820, 41) Pos: 59964 Neg: 239856\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[860]\tvalid_0's auc: 0.642561\n",
            "Bag 2 Fold 1: val ROC AUC 0.6426\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "Early stopping, best iteration is:\n",
            "[740]\tvalid_0's auc: 0.644082\n"
          ]
        }
      ],
      "source": [
        "# improved_pipeline_bagging.py\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import f1_score, precision_recall_curve, roc_auc_score\n",
        "import lightgbm as lgb\n",
        "\n",
        "# ----------------- Конфигурация -----------------\n",
        "DATA_DIR = \".\"\n",
        "TRAIN_FILE = \"X_train.csv\"\n",
        "TEST_FILE = \"X_test.csv\"\n",
        "Y_FILE = \"y_train.csv\"\n",
        "SUBMISSION_FILE = \"submission.csv\"\n",
        "\n",
        "N_BAGS = 5                  # сколько разных undersample-багов\n",
        "RATIO_NEG_PER_POS = 4       # отношение neg:pos в каждом баге (пример: 4 -> 4 negatives на 1 positive)\n",
        "FOLDS = 4                   # Stratified KFold внутри каждого бага для OOF и ранней остановки\n",
        "RANDOM_SEED = 42\n",
        "LGB_PARAMS = {\n",
        "    'objective': 'binary',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'metric': 'auc',\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 64,\n",
        "    'max_depth': -1,\n",
        "    'n_estimators': 4000,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.6,\n",
        "    'min_child_samples': 100,\n",
        "    'reg_alpha': 1.0,\n",
        "    'reg_lambda': 1.0,\n",
        "    'verbosity': -1,\n",
        "    'n_jobs': -1,\n",
        "    'seed': RANDOM_SEED\n",
        "}\n",
        "EARLY_STOPPING_ROUNDS = 100\n",
        "VERBOSE = 100\n",
        "\n",
        "# ----------------- Вспомогательные -----------------\n",
        "def seed_everything(seed=RANDOM_SEED):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "seed_everything()\n",
        "\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    \"\"\"Понизить dtypes для экономии памяти.\"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        if col_type == object:\n",
        "            continue\n",
        "        if str(col_type).startswith('int') or str(col_type).startswith('uint'):\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if c_min >= 0:\n",
        "                if c_max < 255:\n",
        "                    df[col] = df[col].astype(np.uint8)\n",
        "                elif c_max < 65535:\n",
        "                    df[col] = df[col].astype(np.uint16)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.uint32)\n",
        "            else:\n",
        "                if np.iinfo(np.int8).min < c_min < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif np.iinfo(np.int16).min < c_min < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "        else:\n",
        "            # float\n",
        "            df[col] = df[col].astype(np.float32)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose:\n",
        "        print(f\"Mem usage reduced from {start_mem:.2f} MB to {end_mem:.2f} MB\")\n",
        "    return df\n",
        "\n",
        "# ----------------- Feature engineering -----------------\n",
        "def eng_enc_paym_stats(df, prefix='enc_paym_'):\n",
        "    cols = [c for c in df.columns if c.startswith(prefix)]\n",
        "    if not cols:\n",
        "        return pd.DataFrame(index=df.index)\n",
        "    arr = df[cols].fillna(-999).astype(int).values  # -999 for missing\n",
        "    # last non-missing\n",
        "    last = np.full(arr.shape[0], -999, dtype=np.int32)\n",
        "    first = np.full(arr.shape[0], -999, dtype=np.int32)\n",
        "    nuniq = np.zeros(arr.shape[0], dtype=np.int32)\n",
        "    miss_frac = (arr == -999).mean(axis=1).astype(np.float32)\n",
        "    # counts of specific statuses (we don't know cardinality, so count top-k statuses)\n",
        "    # compute last and first efficiently:\n",
        "    valid_mask = (arr != -999)\n",
        "    any_valid = valid_mask.any(axis=1)\n",
        "    # first\n",
        "    idx_first = valid_mask.argmax(axis=1)\n",
        "    first[any_valid] = arr[np.arange(arr.shape[0])[any_valid], idx_first[any_valid]]\n",
        "    # last\n",
        "    rev_idx = valid_mask[:, ::-1].argmax(axis=1)\n",
        "    pos_last = arr.shape[1] - 1 - rev_idx\n",
        "    last[any_valid] = arr[np.arange(arr.shape[0])[any_valid], pos_last[any_valid]]\n",
        "    # unique counts\n",
        "    for i in range(arr.shape[0]):\n",
        "        row = arr[i]\n",
        "        vals = row[row != -999]\n",
        "        if vals.size == 0:\n",
        "            nuniq[i] = 0\n",
        "        else:\n",
        "            nuniq[i] = np.unique(vals).size\n",
        "    # mean/std ignoring -999\n",
        "    arr_float = np.where(arr == -999, np.nan, arr).astype(np.float32)\n",
        "    mean = np.nanmean(arr_float, axis=1)\n",
        "    std = np.nanstd(arr_float, axis=1)\n",
        "    # compute longest consecutive streak of same status (simple)\n",
        "    def longest_streak(row):\n",
        "        prev = None\n",
        "        cur = 0\n",
        "        best = 0\n",
        "        for v in row:\n",
        "            if v == -999:\n",
        "                continue\n",
        "            if prev is None or v != prev:\n",
        "                cur = 1\n",
        "                prev = v\n",
        "            else:\n",
        "                cur += 1\n",
        "            if cur > best:\n",
        "                best = cur\n",
        "        return best\n",
        "    longest = np.array([longest_streak(r) for r in arr], dtype=np.int32)\n",
        "    out = pd.DataFrame({\n",
        "        'enc_first': first,\n",
        "        'enc_last': last,\n",
        "        'enc_n_unique': nuniq,\n",
        "        'enc_missing_frac': miss_frac,\n",
        "        'enc_mean': mean,\n",
        "        'enc_std': std,\n",
        "        'enc_longest_streak': longest\n",
        "    }, index=df.index)\n",
        "    return out\n",
        "\n",
        "def eng_overdues_stats(df, prefix='overdues_'):\n",
        "    cols = [c for c in df.columns if c.startswith(prefix)]\n",
        "    if not cols:\n",
        "        return pd.DataFrame(index=df.index)\n",
        "    arr = df[cols].fillna(0).astype(np.float32).values\n",
        "    total = arr.sum(axis=1)\n",
        "    n_nonzero = (arr > 0).sum(axis=1)\n",
        "    # weighted sum by bucket index (to capture severity)\n",
        "    weights = np.arange(1, arr.shape[1] + 1).astype(np.float32)\n",
        "    weighted = (arr * weights).sum(axis=1)\n",
        "    max_bucket = np.argmax(arr, axis=1)\n",
        "    out = pd.DataFrame({\n",
        "        'over_total': total,\n",
        "        'over_nonzero_cnt': n_nonzero,\n",
        "        'over_weighted': weighted,\n",
        "        'over_max_bucket': max_bucket\n",
        "    }, index=df.index)\n",
        "    return out\n",
        "\n",
        "def eng_no_overdues_stats(df, prefix='no_overdues_'):\n",
        "    cols = [c for c in df.columns if c.startswith(prefix)]\n",
        "    if not cols:\n",
        "        return pd.DataFrame(index=df.index)\n",
        "    arr = df[cols].fillna(0).astype(np.float32).values\n",
        "    total = arr.sum(axis=1)\n",
        "    mean = arr.mean(axis=1)\n",
        "    out = pd.DataFrame({\n",
        "        'no_over_total': total,\n",
        "        'no_over_mean': mean\n",
        "    }, index=df.index)\n",
        "    return out\n",
        "\n",
        "def numeric_interactions(df):\n",
        "    df = df.copy()\n",
        "    EPS = 1e-9\n",
        "    # maturity diff and ratio\n",
        "    if 'maturity_plan' in df.columns and 'maturity_fact' in df.columns:\n",
        "        df['maturity_diff'] = df['maturity_fact'] - df['maturity_plan']\n",
        "        df['maturity_ratio'] = df['maturity_fact'] / (df['maturity_plan'] + EPS)\n",
        "        df['closed_early'] = (df['maturity_fact'] < df['maturity_plan']).astype(np.int8)\n",
        "    if 'sum_left_to_pay' in df.columns and 'credit_limit' in df.columns:\n",
        "        df['utilization'] = df['sum_left_to_pay'] / (df['credit_limit'] + EPS)\n",
        "    if 'next_payment_sum' in df.columns and 'credit_limit' in df.columns:\n",
        "        df['next_payment_ratio'] = df['next_payment_sum'] / (df['credit_limit'] + EPS)\n",
        "    if 'current_overdue_debt' in df.columns:\n",
        "        df['has_current_overdue'] = (df['current_overdue_debt'] > 0).astype(np.int8)\n",
        "    money_cols = ['credit_limit','next_payment_sum','sum_left_to_pay','current_overdue_debt','max_overdue_debt','full_credit_cost']\n",
        "    for c in money_cols:\n",
        "        if c in df.columns:\n",
        "            df[c + '_log1p'] = np.log1p(df[c].fillna(0.0).astype(float))\n",
        "    if 'maturity_plan' in df.columns and 'days_since_confirmed' in df.columns:\n",
        "        df['maturity_remaining'] = df['maturity_plan'] - df['days_since_confirmed']\n",
        "    return df\n",
        "\n",
        "# ----------------- Target encoding -----------------\n",
        "def target_encode_kfold(train_df, target, col, n_splits=5, seed=RANDOM_SEED, min_samples_leaf=100, smoothing=10.0):\n",
        "    \"\"\"\n",
        "    K-fold target encoding with smoothing.\n",
        "    Returns encoded column for train (oof) and mapping to apply on test.\n",
        "    \"\"\"\n",
        "    oof = pd.Series(index=train_df.index, dtype=np.float32)\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
        "    prior = target.mean()\n",
        "    for tr_idx, val_idx in kf.split(train_df, target):\n",
        "        X_tr = train_df.iloc[tr_idx]\n",
        "        y_tr = target.iloc[tr_idx]\n",
        "        stats = y_tr.groupby(X_tr[col]).agg(['mean','count'])\n",
        "        means = stats['mean']\n",
        "        counts = stats['count']\n",
        "        # smoothing\n",
        "        smooth = (counts * means + smoothing * prior) / (counts + smoothing)\n",
        "        mapping = smooth.to_dict()\n",
        "        # map validation\n",
        "        oof.iloc[val_idx] = train_df.iloc[val_idx][col].map(mapping).fillna(prior).astype(np.float32)\n",
        "    # full mapping from full train for test transformation\n",
        "    full_stats = target.groupby(train_df[col]).agg(['mean','count'])\n",
        "    full_smooth = (full_stats['count'] * full_stats['mean'] + smoothing * prior) / (full_stats['count'] + smoothing)\n",
        "    full_map = full_smooth.to_dict()\n",
        "    return oof, full_map, prior\n",
        "\n",
        "# ----------------- Balancing helpers -----------------\n",
        "def sample_negatives_indices(y_series, desired_neg_ratio=RATIO_NEG_PER_POS, random_state=None):\n",
        "    rng = np.random.RandomState(random_state)\n",
        "    idx = y_series.index.values\n",
        "    pos_mask = (y_series.values == 1)\n",
        "    pos_idx = idx[pos_mask]\n",
        "    neg_idx = idx[~pos_mask]\n",
        "    n_pos = pos_idx.shape[0]\n",
        "    n_neg_req = int(n_pos * desired_neg_ratio)\n",
        "    n_neg_req = min(n_neg_req, neg_idx.shape[0])\n",
        "    sampled_neg = rng.choice(neg_idx, size=n_neg_req, replace=False)\n",
        "    selected = np.concatenate([pos_idx, sampled_neg])\n",
        "    rng.shuffle(selected)\n",
        "    return selected\n",
        "\n",
        "# ----------------- Training / OOF / Predict -----------------\n",
        "def find_best_threshold(y_true, probs):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, probs)\n",
        "    f1_scores = 2 * precision * recall / (precision + recall + 1e-12)\n",
        "    best_idx = np.nanargmax(f1_scores)\n",
        "    best_thr = thresholds[best_idx] if best_idx < len(thresholds) else 0.5\n",
        "    return float(best_thr), float(f1_scores[best_idx])\n",
        "\n",
        "def train_bagging_ensemble(X, y, X_test, cat_feats=None,\n",
        "                           n_bags=N_BAGS, ratio_neg=RATIO_NEG_PER_POS,\n",
        "                           folds=FOLDS, seed=RANDOM_SEED):\n",
        "    \"\"\"\n",
        "    Обучаем ансамбль из n_bags: в каждом баге делаем undersample негативов, затем внутри StratifiedKFold.\n",
        "    Возвращаем список моделей, ооф (усредненный по багам), и средние прогнозы на тест.\n",
        "    \"\"\"\n",
        "    n_train = X.shape[0]\n",
        "    oof_preds_accum = np.zeros(n_train, dtype=np.float32)\n",
        "    test_preds_accum = np.zeros(X_test.shape[0], dtype=np.float32)\n",
        "    total_models = 0\n",
        "    models = []\n",
        "\n",
        "    for b in range(n_bags):\n",
        "        print(f\"\\n=== BAG {b+1}/{n_bags} ===\")\n",
        "        sel_idx = sample_negatives_indices(y, desired_neg_ratio=ratio_neg, random_state=seed + b)\n",
        "        X_b = X.loc[sel_idx]\n",
        "        y_b = y.loc[sel_idx]\n",
        "        print(\"Bag train shape:\", X_b.shape, \"Pos:\", int(y_b.sum()), \"Neg:\", int((y_b==0).sum()))\n",
        "\n",
        "        # inside bag CV\n",
        "        skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed + b)\n",
        "        oof_b = np.zeros(n_train, dtype=np.float32)  # will write to positions for validation folds only\n",
        "        for fold, (tr_idx_local, val_idx_local) in enumerate(skf.split(X_b, y_b)):\n",
        "            tr_idx = sel_idx[tr_idx_local]  # original indices\n",
        "            val_idx = sel_idx[val_idx_local]\n",
        "            X_tr, X_val = X.loc[tr_idx], X.loc[val_idx]\n",
        "            y_tr, y_val = y.loc[tr_idx], y.loc[val_idx]\n",
        "\n",
        "            lgb_train = lgb.Dataset(X_tr, label=y_tr, categorical_feature=cat_feats if cat_feats else 'auto', free_raw_data=False)\n",
        "            lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train, categorical_feature=cat_feats if cat_feats else 'auto', free_raw_data=False)\n",
        "\n",
        "            params = LGB_PARAMS.copy()\n",
        "            # scale_pos_weight might help when within-bag still imbalanced\n",
        "            pos = int(y_tr.sum())\n",
        "            neg = int(y_tr.shape[0] - pos)\n",
        "            if pos > 0:\n",
        "                spw = max(1.0, neg / (pos + 1e-9))\n",
        "                params['scale_pos_weight'] = spw\n",
        "\n",
        "            model = lgb.train(\n",
        "                params=params,\n",
        "                train_set=lgb_train,\n",
        "                valid_sets=[lgb_val],\n",
        "                num_boost_round=1000,\n",
        "                callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
        "            )\n",
        "            # val predictions (on original train index positions)\n",
        "            val_pred = model.predict(X.loc[val_idx], num_iteration=model.best_iteration)\n",
        "            oof_b[val_idx] = val_pred\n",
        "            # add test predictions\n",
        "            test_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
        "            test_preds_accum += test_pred\n",
        "            total_models += 1\n",
        "            models.append(model)\n",
        "            print(f\"Bag {b+1} Fold {fold+1}: val ROC AUC {roc_auc_score(y.loc[val_idx], val_pred):.4f}\")\n",
        "            gc.collect()\n",
        "\n",
        "        # accumulate oof from this bag (we average across bags later)\n",
        "        oof_preds_accum += oof_b\n",
        "\n",
        "    # average oof and test\n",
        "    oof_preds = oof_preds_accum / n_bags\n",
        "    test_preds = test_preds_accum / total_models  # because we added test prediction per model\n",
        "    print(\"\\nTotal models trained:\", total_models)\n",
        "    return models, oof_preds, test_preds\n",
        "\n",
        "# ----------------- Main pipeline -----------------\n",
        "def main():\n",
        "    required = [os.path.join(DATA_DIR, TRAIN_FILE), os.path.join(DATA_DIR, TEST_FILE), os.path.join(DATA_DIR, Y_FILE)]\n",
        "    if not all(os.path.exists(p) for p in required):\n",
        "        print(\"Не найдены входные файлы. Поместите X_train.csv, X_test.csv, y_train.csv в текущую папку.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(\"Loading...\")\n",
        "    X_train = pd.read_csv(os.path.join(DATA_DIR, TRAIN_FILE))\n",
        "    X_test = pd.read_csv(os.path.join(DATA_DIR, TEST_FILE))\n",
        "    y_df = pd.read_csv(os.path.join(DATA_DIR, Y_FILE))\n",
        "\n",
        "    # Находим столбец флага\n",
        "    if 'flag' in y_df.columns:\n",
        "        y = y_df['flag'].astype(int)\n",
        "    else:\n",
        "        y = y_df.iloc[:,0].astype(int)\n",
        "    print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape, \"Y shape:\", y.shape)\n",
        "    print(\"Target distribution:\", y.value_counts(normalize=True).to_dict())\n",
        "\n",
        "    # Сохраним id, если есть\n",
        "    id_col = 'id' if 'id' in X_test.columns else None\n",
        "\n",
        "    # Feature engineering\n",
        "    print(\"Feature engineering...\")\n",
        "    # numeric interactions\n",
        "    Xtr_num = numeric_interactions(X_train)\n",
        "    Xte_num = numeric_interactions(X_test)\n",
        "    # enc_paym stats\n",
        "    enc_tr = eng_enc_paym_stats(Xtr_num, prefix='enc_paym_')\n",
        "    enc_te = eng_enc_paym_stats(Xte_num, prefix='enc_paym_')\n",
        "    # overdues\n",
        "    over_tr = eng_overdues_stats(Xtr_num, prefix='overdues_')\n",
        "    over_te = eng_overdues_stats(Xte_num, prefix='overdues_')\n",
        "    # no overdues\n",
        "    noover_tr = eng_no_overdues_stats(Xtr_num, prefix='no_overdues_')\n",
        "    noover_te = eng_no_overdues_stats(Xte_num, prefix='no_overdues_')\n",
        "\n",
        "    # Drop raw big blocks to save memory\n",
        "    drop_prefixes = ['enc_paym_', 'overdues_', 'no_overdues_']\n",
        "    to_drop_tr = [c for c in Xtr_num.columns if any(c.startswith(p) for p in drop_prefixes)]\n",
        "    to_drop_te = [c for c in Xte_num.columns if any(c.startswith(p) for p in drop_prefixes)]\n",
        "    Xtr_small = Xtr_num.drop(columns=to_drop_tr, errors='ignore').copy()\n",
        "    Xte_small = Xte_num.drop(columns=to_drop_te, errors='ignore').copy()\n",
        "\n",
        "    # concat engineered features\n",
        "    Xtr_small = pd.concat([Xtr_small, enc_tr, over_tr, noover_tr], axis=1)\n",
        "    Xte_small = pd.concat([Xte_small, enc_te, over_te, noover_te], axis=1)\n",
        "\n",
        "    # Label encode small categoricals then target encode with kfold smoothing\n",
        "    cat_cols = [c for c in ['credit_type','credit_currency'] if c in Xtr_small.columns]\n",
        "    for c in cat_cols:\n",
        "        Xtr_small[c] = Xtr_small[c].fillna(\"nan\").astype(str)\n",
        "        Xte_small[c] = Xte_small[c].fillna(\"nan\").astype(str)\n",
        "        le = LabelEncoder()\n",
        "        le.fit(list(Xtr_small[c].values) + list(Xte_small[c].values))\n",
        "        Xtr_small[c + '_le'] = le.transform(Xtr_small[c])\n",
        "        Xte_small[c + '_le'] = le.transform(Xte_small[c])\n",
        "        # target encoding (KFold)\n",
        "        oof_te, map_full, prior = target_encode_kfold(Xtr_small, y, c, n_splits=5, seed=RANDOM_SEED, smoothing=20.0)\n",
        "        Xtr_small[c + '_te'] = oof_te\n",
        "        Xte_small[c + '_te'] = Xte_small[c].map(map_full).fillna(prior).astype(np.float32)\n",
        "    # drop original text categorical columns\n",
        "    Xtr_small.drop(columns=cat_cols, inplace=True, errors='ignore')\n",
        "    Xte_small.drop(columns=cat_cols, inplace=True, errors='ignore')\n",
        "\n",
        "    # Impute missing values (median)\n",
        "    imputer = SimpleImputer(strategy='median')\n",
        "    Xtr_small.iloc[:,:] = imputer.fit_transform(Xtr_small)\n",
        "    Xte_small.iloc[:,:] = imputer.transform(Xte_small)\n",
        "\n",
        "    # Reduce memory\n",
        "    Xtr_small = reduce_mem_usage(Xtr_small)\n",
        "    Xte_small = reduce_mem_usage(Xte_small)\n",
        "\n",
        "    # Align indices with y\n",
        "    Xtr_small = Xtr_small.loc[y.index]\n",
        "\n",
        "    # Select categorical features for LightGBM (use _le columns if any)\n",
        "    cat_feats = [c for c in Xtr_small.columns if c.endswith('_le')]\n",
        "    print(\"Final feature shape:\", Xtr_small.shape, \"Categorical features:\", cat_feats)\n",
        "\n",
        "    # Train bagging ensemble\n",
        "    print(\"Training bagging ensemble...\")\n",
        "    t0 = time.time()\n",
        "    models, oof_preds, test_preds = train_bagging_ensemble(Xtr_small, y, Xte_small, cat_feats,\n",
        "                                                           n_bags=N_BAGS, ratio_neg=RATIO_NEG_PER_POS,\n",
        "                                                           folds=FOLDS, seed=RANDOM_SEED)\n",
        "    t1 = time.time()\n",
        "    print(f\"Training finished in {(t1-t0)/60:.2f} min\")\n",
        "\n",
        "    # threshold tuning on oof\n",
        "    best_thr, best_f1 = find_best_threshold(y.values, oof_preds)\n",
        "    print(\"Best threshold on OOF:\", best_thr, \"Best OOF F1:\", best_f1)\n",
        "    # final predicted labels\n",
        "    test_labels = (test_preds >= best_thr).astype(int)\n",
        "\n",
        "    # Build submission\n",
        "    if id_col and id_col in X_test.columns:\n",
        "        ids = X_test[id_col].values\n",
        "    else:\n",
        "        ids = np.arange(len(test_labels))\n",
        "\n",
        "    submission = pd.DataFrame({'id': ids, 'flag': test_labels})\n",
        "    submission.to_csv(SUBMISSION_FILE, index=False)\n",
        "    print(\"Saved\", SUBMISSION_FILE, \"shape:\", submission.shape)\n",
        "    # report OOF metrics\n",
        "    print(\"OOF ROC AUC:\", roc_auc_score(y, oof_preds))\n",
        "    print(\"OOF F1 at best_thr:\", f1_score(y, (oof_preds >= best_thr).astype(int)))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QWtnC9DlIPLU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}